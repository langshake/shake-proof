# ğŸŒ Shakeproof Benchmark CLI

**Shakeproof Benchmark CLI** is an open-source benchmarking tool designed to compare **traditional web scraping** against the **LangShake Protocol**â€”a new standard for AI-optimized, verifiable, structured content delivery.

> This project empowers developers, webmasters, and AI platform integrators to validate, trust, and optimize the way web data is shared with LLMs and agents.

## ğŸš€ What Is LangShake?

**LangShake** introduces `.well-known/llm.json` and per-page JSON modules to allow any website to expose **clean**, **verifiable**, **schema.org-compliant** dataâ€”without bloating the HTML or risking misinterpretation.

This CLI measures:
- **Extraction accuracy**
- **Speed**
- **Trustworthiness** (via checksum & Merkle tree validation)
- **Real-world crawl performance** (across static and dynamic content)

## ğŸ§© Features

### âœ… Compare Crawling Methods
- **Traditional Scraping**: Extracts Schema.org data from raw HTML (including dynamic React/Next.js content).
- **LangShake Protocol**: Uses `.well-known/llm.json` and verified JSON modules for direct data access.

### ğŸ” Validates Integrity
- Verifies each JSON module's **SHA-256 checksum**
- Recalculates and confirms the **Merkle root** from all modules

### ğŸ“Š Benchmarking Metrics
- Extraction time (per page and per method)
- Schema match validation
- Trust pass/fail reports
- Resource usage metrics (CPU, memory, bandwidth)

### ğŸ§ª Extensive Testing
- Fixture-driven Vitest test suite (real extraction, error cases, checksum logic)
- CLI and SDK are covered with integration and unit tests

## ğŸ›  Installation

```bash
git clone https://github.com/langshake/shake-proof
cd shake-proof
npm install
npm link   # For global CLI access (development)
```

## âš™ï¸ Usage

### CLI

```bash
shakeproof --url https://example.com --method both --json
```

Options:

* `--url <target>`: Required. Domain or full URL to benchmark.
* `--method <type>`: `traditional`, `langshake`, or `both` (default: both)
* `--json`: Outputs structured results as JSON
* `--output <file>`: Save output to file (default: `output/<domain>.json`)
* `--concurrency <num>`: Max parallel page fetches (default: 5)

### SDK

```js
import { runBenchmark } from 'shakeproof-benchmark';

const result = await runBenchmark({
  url: 'https://example.com',
  method: 'both',
  debug: true
});

console.log(result.json);  // Machine-readable
console.log(result.human); // Human-readable summary
```

## ğŸ“¤ Output Format (JSON)

```json
{
  "domainRoot": "https://example.com",
  "pages": [
    {
      "url": "https://example.com/page1",
      "langshake": [ { /* ...schema.org data... */ } ],
      "traditional": [ { /* ...schema.org data... */ } ],
      "comparison": {
        "schemasMatch": true,
        "langshakeChecksum": "...",
        "langshakeChecksumOriginal": "...",
        "langshakeChecksumValid": true,
        "traditionalChecksum": "...",
        "traditionalChecksumMatchesLangshake": true
      }
    }
  ],
  "summary": {
    "totalPages": 8,
    "allMatch": true,
    "details": "All schemas match.",
    "merkleRootLangshake": "...",
    "merkleRootTraditional": "...",
    "merkleRootLlmJson": "...",
    "merkleRootLangshakeValid": true,
    "merkleRootTraditionalValid": true,
    "merkleRootsMatch": true
  },
  "metrics": {
    "langshake": { /* ... metrics data ... */ },
    "traditional": { /* ... metrics data ... */ }
  }
}
```

## ğŸ“¦ Architecture Overview

```
shake-proof/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawlers/
â”‚   â”‚   â”œâ”€â”€ traditional.js    # HTML-based extraction (Cheerio + Selenium)
â”‚   â”‚   â””â”€â”€ langshake.js      # JSON-based validation with Merkle tree
â”‚   â”œâ”€â”€ benchmark/
â”‚   â”‚   â””â”€â”€ compare.js        # Domain-wide and per-page benchmarking logic
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ generateReport.js # Markdown/HTML report generation
â”‚   â”‚   â”œâ”€â”€ merkle.js         # Checksum and Merkle root utilities
â”‚   â”‚   â””â”€â”€ metrics.js        # Resource usage and metrics collection
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â””â”€â”€ menu.js           # CLI entry and argument parsing
â”‚   â””â”€â”€ index.js              # SDK entry point (runBenchmark)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ crawlers/
â”‚   â”‚   â”œâ”€â”€ traditional.test.js
â”‚   â”‚   â””â”€â”€ langshake.test.js
â”‚   â”œâ”€â”€ benchmark/
â”‚   â”‚   â””â”€â”€ compare.test.js
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ generateReport.test.js
â”‚   â”‚   â””â”€â”€ metrics.test.js
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â””â”€â”€ menu.test.js
â”‚   â””â”€â”€ fixtures/
â”‚       â”œâ”€â”€ traditional/      # HTML fixture files for traditional crawler
â”‚       â””â”€â”€ langshake/        # JSON fixture files for langshake protocol
```

## ğŸ“ˆ Metrics Collected

| Category     | Metric                                                                                 |
| ------------ | --------------------------------------------------------------------------------------- |
| âš¡ Speed      | Avg page extraction time, total duration, requests per second (RPS)                    |
| ğŸ§  Accuracy   | Schema match (true/false), extraction correctness                                      |
| ğŸ” Trust      | Checksum/Merkle root verification, validation status                                   |
| ğŸ“Š Resources  | CPU usage (user/system), memory usage (start/end/peak), network (bytes in/out), disk I/O |
| ğŸŒ Network    | HTTP status codes, total requests, average request time                                |
| â— Errors     | Error count, error details (per URL and message)                                        |
| ğŸ§µ Concurrency| Max parallel requests observed                                                          |

## ğŸ§ª Testing

Run all tests:

```bash
npm test
```

Test coverage includes:

* Traditional extraction (static + dynamic HTML)
* LangShake crawler (checksum, malformed JSON, Merkle validation)
* Benchmark engine (pass/fail cases, mixed outcomes)
* CLI user flows (mocked)
* Fixture checksum recalculation

## ğŸ“˜ About the LangShake Protocol

LangShake is a dual-layer micro-standard for machine-readable web content:

* **.well-known/llm.json**: Declares site-wide structured data modules & metadata
* **Modular JSON files**: Contain pure, schema.org-compliant JSON-LD arrays with checksums
* **Merkle root validation**: Ensures integrity across modules

Learn more: [whitepaper](https://github.com/langshake/langshake.github.io/blob/master/whitepaper.md)

## ğŸ§° Companion Tool: LangshakeIt CLI

To generate `.well-known/llm.json` and the per-page JSON-LD modules used by this benchmark tool, use our sister project: **[LangshakeIt CLI](https://github.com/langshake/langshakeit)**.

LangshakeIt is the easiest way to make your website AI- and LLM-friendly by extracting and publishing structured, verifiable data for every page.

### ğŸ”§ What It Does

- Extracts **Schema.org-compliant JSON-LD** from your built static site (no framework lock-in)
- Outputs **per-page JSON** files (with checksums) and a global `.well-known/llm.json` index
- Automatically calculates and embeds a **Merkle root** to ensure integrity
- Supports **optional LLM context** via `llm_context.json` (e.g., ethical principles, usage notes)
- Includes smart caching and auto-detection of your site's public base URL

## ğŸŒ Get Involved

LangShake is fully open source (MIT) and community-driven.

We welcome:

* Web developers who want to expose AI-friendly content
* Toolmakers who want to integrate LangShake support
* Contributors to help expand crawler compatibility or reporting

ğŸ‘‰ GitHub: [github.com/langshake](https://github.com/langshake)

## ğŸ§­ Roadmap

* [ ] Add resource usage and impact profiling (CPU, memory)
* [ ] Support fallback sitemaps when `.llm.json` is missing
* [ ] Integrate with third-party SEO tools
* [ ] Submit LangShake Sitemap extension to W3C

## ğŸ“ License

MIT â€” Free to use, fork, improve, and adapt.

## ğŸ¤ Thanks

This project was inspired by the growing need for **verifiable**, **trustworthy**, and **machine-optimized** content delivery. We believe LangShake can be the `robots.txt` of the AI era.